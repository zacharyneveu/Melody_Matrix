{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import soundcard as sc\n",
    "import torchaudio as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from note_model import *\n",
    "from zach_visualization import *\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def mel_scale(data, SR=16000, NFFT=4095, N_MELS=256):\n",
    "    \"\"\" converts spectrum to MEL spectrum without using librosa/torchaudio because they're broken on Jetson\n",
    "        Note: only works with single channel\n",
    "        Arguments:\n",
    "            SR: sample rate\n",
    "            NFFT: length of FFT (except *2-1 because of implementation?)\n",
    "            N_MELS: size of output vector\n",
    "    \"\"\"\n",
    "    low_freq_mel = 0\n",
    "    high_freq_mel = (2595 * np.log10(1 + (SR / 2) / 700))  # Convert Hz to Mel\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, N_MELS + 2)  # Equally spaced in Mel scale\n",
    "    hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\n",
    "    bin = np.floor((NFFT + 1) * hz_points / SR)\n",
    "\n",
    "    fbank = np.zeros((N_MELS, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, N_MELS + 1):\n",
    "        f_m_minus = int(bin[m - 1])   # left\n",
    "        f_m = int(bin[m])             # center\n",
    "        f_m_plus = int(bin[m + 1])    # right\n",
    "\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = np.dot(data, fbank.T)\n",
    "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Numerical Stability\n",
    "    filter_banks = 20 * np.log10(filter_banks)  # dB\n",
    "    return filter_banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "mic = sc.get_microphone('CODEC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "SR = 16000     # highest piano note is ~4k, shouldn't need more than double that range\n",
    "NMELS = 256    # set by the model, don't change here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Audio2Midi(\n",
       "  (input): Conv1d(1, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(64, 128, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Tanh()\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(128, 64, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Tanh()\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(64, 32, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Tanh()\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Tanh()\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=88, bias=True)\n",
       "  (rnn): GRU(128, 88)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Audio2Midi(kernel_size=9)\n",
    "model.load_state_dict(torch.load(\"./models/300_2e-05.pth\"))\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to:  /dev/ttyACM1 \n",
      "...\n",
      "Connected to:  /dev/ttyACM1  !\n"
     ]
    }
   ],
   "source": [
    "c = Cube(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def make_frame(l_vec, r_vec, genre, thresh):\n",
    "    \"\"\"\n",
    "    Creates a frame for visualization stack out of left and right machine learning outputs\n",
    "    Arguments:\n",
    "        l_vec: velocity vector for left channel\n",
    "        r_vec: velocity vector for right channel\n",
    "        genre: genre to pass through to frame\n",
    "        thresh: velocity value under which to ignore note\n",
    "    \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    pans = torch.round(63.5 + torch.clamp(r_vec / l_vec, -63, 63))\n",
    "    velocities = torch.max(torch.cat((l_vec.unsqueeze(1), r_vec.unsqueeze(1)), dim=1), dim=1).values\n",
    "    for pitch in range(velocities.shape[0]):\n",
    "        if 127 >= velocities[pitch].item() >= thresh:\n",
    "            notes.append(Note(velocities[pitch].item(), pitch, pans[pitch].item()))\n",
    "\n",
    "    frame = Frame(notes, genre)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Using ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Melody_Matrix/maching_learning/note_classifier/zach_visualization.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[0mnparr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords2Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marduino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnparr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Melody_Matrix/maching_learning/note_classifier/Arduino.py\u001b[0m in \u001b[0;36msendArray\u001b[0;34m(self, colorArray)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolorArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_v2/lib/python3.7/site-packages/serial/serialposix.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;31m# wait for write operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \u001b[0mabort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_abort_write_r\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mabort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_abort_write_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g = Genre('default')\n",
    "mean_val = 0;\n",
    "with mic.recorder(samplerate=SR, blocksize=32) as m:\n",
    "    #for j in range(SR//2048*10):\n",
    "    while True:\n",
    "        data = m.record(numframes=2048)\n",
    "        data /= np.max(np.abs(data))\n",
    "        fframe = np.abs(np.fft.rfft(data))\n",
    "        fframe = np.log(np.clip(fframe, 1e-5, None))\n",
    "        lframe = mel_scale(np.abs(fframe[:,0]))\n",
    "        rframe = mel_scale(np.abs(fframe[:,1]))\n",
    "        mean_val = np.mean(lframe)\n",
    "        lout = model(torch.from_numpy(lframe).float().unsqueeze(0).unsqueeze(0).cuda())\n",
    "        rout = model(torch.from_numpy(rframe).float().unsqueeze(0).unsqueeze(0).cuda())\n",
    "        f = make_frame(lout.detach(), rout.detach(), g, thresh=0.1)\n",
    "        c.process_frame(f)\n",
    "        #ipd.clear_output()\n",
    "        c.display()\n",
    "        #display(c.display_screen(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Using Raw Mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Melody_Matrix/maching_learning/note_classifier/zach_visualization.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[0mnparr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords2Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marduino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnparr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Melody_Matrix/maching_learning/note_classifier/Arduino.py\u001b[0m in \u001b[0;36msendArray\u001b[0;34m(self, colorArray)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtestAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_leds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g = Genre('default')\n",
    "mean_val = 0;\n",
    "with mic.recorder(samplerate=SR, blocksize=32) as m:\n",
    "    #for j in range(SR//2048*30):\n",
    "    while True:\n",
    "        data = m.record(numframes=2048)\n",
    "        data /= np.max(np.abs(data))\n",
    "        fframe = np.abs(np.fft.rfft(data))\n",
    "        fframe = np.log(np.clip(fframe, 1e-5, None))\n",
    "        lframe = mel_scale(np.abs(fframe[:,0]))\n",
    "        rframe = mel_scale(np.abs(fframe[:,1]))\n",
    "        mean_val = np.mean(lframe)\n",
    "        offset = 20\n",
    "        lout = torch.from_numpy(lframe[offset:128+offset])\n",
    "        rout = torch.from_numpy(rframe[offset:128+offset])\n",
    "        f = make_frame(lout.detach(), rout.detach(), g, thresh=25)\n",
    "        c.process_frame(f)\n",
    "        #ipd.clear_output()\n",
    "        c.display()\n",
    "        #display(c.display_screen(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.351149030321936"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "c.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "?torch.clamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 12\n",
      "i: 13\n",
      "i: 14\n",
      "i: 15\n",
      "i: 16\n",
      "i: 17\n",
      "i: 18\n",
      "i: 19\n",
      "i: 20\n",
      "i: 21\n",
      "i: 22\n",
      "i: 23\n",
      "i: 24\n",
      "i: 25\n",
      "i: 26\n",
      "i: 27\n",
      "i: 28\n",
      "i: 29\n",
      "i: 30\n",
      "i: 31\n",
      "i: 32\n",
      "i: 33\n",
      "i: 34\n",
      "i: 35\n",
      "i: 36\n",
      "i: 37\n",
      "i: 38\n",
      "i: 39\n",
      "i: 40\n",
      "i: 41\n",
      "i: 42\n",
      "i: 43\n",
      "i: 44\n",
      "i: 45\n",
      "i: 46\n",
      "i: 47\n",
      "i: 48\n",
      "i: 49\n",
      "i: 50\n",
      "i: 51\n",
      "i: 52\n",
      "i: 53\n",
      "i: 54\n",
      "i: 55\n",
      "i: 56\n",
      "i: 57\n",
      "i: 58\n",
      "i: 59\n",
      "i: 60\n",
      "i: 61\n",
      "i: 62\n",
      "i: 63\n",
      "i: 64\n",
      "i: 65\n",
      "i: 66\n",
      "i: 67\n",
      "i: 68\n",
      "i: 69\n",
      "i: 70\n",
      "i: 71\n",
      "i: 72\n",
      "i: 73\n",
      "i: 74\n",
      "i: 75\n",
      "i: 76\n",
      "i: 77\n",
      "i: 78\n",
      "i: 79\n",
      "i: 80\n",
      "i: 81\n",
      "i: 82\n",
      "i: 83\n",
      "i: 84\n",
      "i: 85\n",
      "i: 86\n",
      "i: 87\n",
      "i: 88\n",
      "i: 89\n",
      "i: 90\n",
      "i: 91\n",
      "i: 92\n",
      "i: 93\n",
      "i: 94\n",
      "i: 95\n",
      "i: 96\n",
      "i: 97\n",
      "i: 98\n",
      "i: 99\n",
      "i: 100\n",
      "i: 101\n",
      "i: 102\n",
      "i: 103\n",
      "i: 104\n",
      "i: 105\n",
      "i: 106\n",
      "i: 107\n",
      "i: 108\n",
      "i: 109\n",
      "i: 110\n",
      "i: 111\n",
      "i: 112\n",
      "i: 113\n",
      "i: 114\n",
      "i: 115\n",
      "i: 116\n",
      "i: 117\n",
      "i: 118\n",
      "i: 119\n",
      "i: 120\n",
      "i: 121\n",
      "i: 122\n",
      "i: 123\n",
      "i: 124\n",
      "i: 125\n",
      "i: 126\n"
     ]
    }
   ],
   "source": [
    "#notes = [Note(64,i,64) for i in range(0, 72+0)]\n",
    "for i in range(12, 127):\n",
    "    print(f\"i: {i}\")\n",
    "    notes = [Note(127, i, 64)]\n",
    "    f = Frame(notes, Genre('default'))\n",
    "\n",
    "    c.clear()\n",
    "    c.process_frame(f)\n",
    "    c.display()\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "c.arduino.testAnimation(c.size**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
